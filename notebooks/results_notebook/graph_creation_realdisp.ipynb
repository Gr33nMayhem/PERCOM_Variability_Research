{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-25T11:38:30.051865Z",
     "end_time": "2024-03-25T11:38:32.368762Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "root_path = os.path.abspath(os.path.join('..', '..'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-25T11:38:32.369762Z",
     "end_time": "2024-03-25T11:38:32.380765Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Method used for getting the right path for a model based on the device name and the cv number\n",
    "'''\n",
    "root_path: the root path of the project\n",
    "device: the device name\n",
    "cv: the cross validation number\n",
    "\n",
    "Acceptable model types: attend, deepconv, tinyhar\n",
    "'''\n",
    "\n",
    "\n",
    "def get_model_path(root_path, variant, model_type, cv):\n",
    "    path = root_path\n",
    "    parent_path = os.path.join(path, 'data', 'Run_logs', variant, 'logs')\n",
    "    # parent paths has three folders for attend, deepCONV, TinyHAR. Get a list of all folders in the parent path\n",
    "    folders = os.listdir(parent_path)\n",
    "    for folder in folders:\n",
    "        if model_type in folder:\n",
    "            path = os.path.join(parent_path, folder)\n",
    "            break\n",
    "    # path now has the path to the model type folder\n",
    "    path = os.path.join(path, 'cv_' + str(cv))\n",
    "    return path\n",
    "\n",
    "\n",
    "def get_result_for_model(root_path, model_variant, test_variant, model_type, cv):\n",
    "    path = get_model_path(root_path, model_variant, model_type, cv)\n",
    "    filename = 'prediction_result_' + model_variant + '_' + test_variant + '.csv'\n",
    "    path = os.path.join(path, filename)\n",
    "    # open the csv file prediction_result_ and read the data\n",
    "    csv_file = path\n",
    "    # open with pandas\n",
    "    df = pd.read_csv(csv_file)\n",
    "    # if the file is empty, continue\n",
    "    if df.empty:\n",
    "        return None\n",
    "\n",
    "    # get the prediction result\n",
    "    preds = df['preds'].values\n",
    "    # get the ground truth\n",
    "    trues = df['trues'].values\n",
    "\n",
    "    # Assuming binary classification\n",
    "    num_classes = 2\n",
    "\n",
    "    # Calculate precision, recall, and f1 for each class\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(trues, preds, average=None)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = np.sum(preds == trues) / len(preds)\n",
    "\n",
    "    # Calculate macro F1\n",
    "    f1_macro = np.mean(f1)\n",
    "\n",
    "    # Calculate weighted F1\n",
    "    class_distribution = np.bincount(trues)\n",
    "    f1_weighted = np.sum(f1 * class_distribution / len(trues))\n",
    "\n",
    "    # accuracy = np.sum(preds == trues) / len(preds)\n",
    "    # precision = np.sum((preds == 1) & (trues == 1)) / np.sum(preds == 1)\n",
    "    # recall = np.sum((preds == 1) & (trues == 1)) / np.sum(trues == 1)\n",
    "    # f1 = 2 * np.sum((preds == 1) & (trues == 1)) / (np.sum(preds == 1) + np.sum(trues == 1))\n",
    "    # f1_w = (precision + recall) / 2\n",
    "\n",
    "    return accuracy, precision, recall, f1_macro, f1_weighted  #, preds, trues"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-25T11:44:36.230793Z",
     "end_time": "2024-03-25T11:44:36.249792Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1 v 1 Graphs and Info"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\Concordia\\\\PERCOM_Variability_Model_Research\\\\data\\\\Run_logs\\\\RLA-ideal\\\\logs\\\\attend_data_realdisp_seed_1_windowsize_100_waveFilter_False_Fscaling_1_cvfilter_64_grufilter_128_Regu_False_wavelearnble_False\\\\cv_0\\\\prediction_result_RLA-ideal_self.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m17\u001B[39m):\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m model \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mattend\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdeepconv\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtinyhar\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[1;32m----> 5\u001B[0m         result1 \u001B[38;5;241m=\u001B[39m \u001B[43mget_result_for_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mroot_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mRLA-ideal\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mself\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m         result2 \u001B[38;5;241m=\u001B[39m get_result_for_model(root_path, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRLA-self\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mself\u001B[39m\u001B[38;5;124m'\u001B[39m, model, i)\n\u001B[0;32m      7\u001B[0m         dict_results \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mf1\u001B[39m\u001B[38;5;124m'\u001B[39m: result2[\u001B[38;5;241m3\u001B[39m],\n\u001B[0;32m      8\u001B[0m                         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcv\u001B[39m\u001B[38;5;124m'\u001B[39m: i,\n\u001B[0;32m      9\u001B[0m                         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdevice_config\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mself vs self\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel_type\u001B[39m\u001B[38;5;124m'\u001B[39m: model}\n",
      "Cell \u001B[1;32mIn[7], line 32\u001B[0m, in \u001B[0;36mget_result_for_model\u001B[1;34m(root_path, model_variant, test_variant, model_type, cv)\u001B[0m\n\u001B[0;32m     30\u001B[0m csv_file \u001B[38;5;241m=\u001B[39m path\n\u001B[0;32m     31\u001B[0m \u001B[38;5;66;03m# open with pandas\u001B[39;00m\n\u001B[1;32m---> 32\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcsv_file\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;66;03m# if the file is empty, continue\u001B[39;00m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m df\u001B[38;5;241m.\u001B[39mempty:\n",
      "File \u001B[1;32mD:\\Concordia\\PERCOM_Variability_Model_Research\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1024\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[0;32m   1011\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m   1012\u001B[0m     dialect,\n\u001B[0;32m   1013\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1020\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m   1021\u001B[0m )\n\u001B[0;32m   1022\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m-> 1024\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Concordia\\PERCOM_Variability_Model_Research\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:618\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    615\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    617\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 618\u001B[0m parser \u001B[38;5;241m=\u001B[39m TextFileReader(filepath_or_buffer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    620\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    621\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32mD:\\Concordia\\PERCOM_Variability_Model_Research\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1618\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m   1615\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m   1617\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1618\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Concordia\\PERCOM_Variability_Model_Research\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1878\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1876\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[0;32m   1877\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1878\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1879\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1880\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1881\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1882\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1883\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1884\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1885\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1886\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1887\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1888\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1889\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[1;32mD:\\Concordia\\PERCOM_Variability_Model_Research\\venv\\lib\\site-packages\\pandas\\io\\common.py:873\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    868\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    869\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    870\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    871\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m    872\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[1;32m--> 873\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    874\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    875\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    876\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    877\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    878\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    879\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    880\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    881\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m    882\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'D:\\\\Concordia\\\\PERCOM_Variability_Model_Research\\\\data\\\\Run_logs\\\\RLA-ideal\\\\logs\\\\attend_data_realdisp_seed_1_windowsize_100_waveFilter_False_Fscaling_1_cvfilter_64_grufilter_128_Regu_False_wavelearnble_False\\\\cv_0\\\\prediction_result_RLA-ideal_self.csv'"
     ]
    }
   ],
   "source": [
    "# Blue sense rwr1 vs rwr2\n",
    "result_list = []\n",
    "for i in range(17):\n",
    "    for model in ['attend', 'deepconv', 'tinyhar']:\n",
    "        result1 = get_result_for_model(root_path, 'RLA-ideal', 'self', model, i)\n",
    "        result2 = get_result_for_model(root_path, 'RLA-self', 'self', model, i)\n",
    "        dict_results = {'f1': result2[3],\n",
    "                        'cv': i,\n",
    "                        'device_config': 'self vs self', 'model_type': model}\n",
    "        result_list.append(dict_results)\n",
    "        dict_results = {'f1': result1[3],\n",
    "                        'cv': i,\n",
    "                        'device_config': 'ideal vs ideal', 'model_type': model}\n",
    "        result_list.append(dict_results)\n",
    "\n",
    "df = pd.DataFrame(result_list)\n",
    "# print only rwr1 vs rwr1 and model deepconv\n",
    "print(df[(df['device_config'] == 'self vs ideal') & (df['model_type'] == 'tinyhar')])\n",
    "\n",
    "# plot the f1 score for all participants as bar graph using sns catplot bar\n",
    "sns.catplot(x='cv', y='f1', hue='device_config', data=df, kind='bar', aspect=2, height=5)\n",
    "plt.title('RLA ideal vs self')\n",
    "plt.xlabel('Cross Validation')\n",
    "plt.ylabel('F1 Score')\n",
    "# plot max y value is 1.1\n",
    "plt.ylim(0, 1.1)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-11T10:25:28.577348Z",
     "end_time": "2024-03-11T10:25:29.827977Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Method for creating a multi bar graph with x axis and y axis values in the for of a list\n",
    "'''\n",
    "x: list of x axis labels\n",
    "y: list of y axis values\n",
    "x_label: x axis label\n",
    "y_label: y axis label\n",
    "title: title of the graph\n",
    "width: width of the graph\n",
    "height: height of the graph\n",
    "bar_width: width of each bar\n",
    "'''\n",
    "\n",
    "\n",
    "def generate_multi_bar_graph(data_frame, x_label, y_label, title, width, height, bar_width, std=False):\n",
    "    # set the width of the bars\n",
    "    bar_width = bar_width\n",
    "    sns.barplot(data_frame, x=x_label, y=y_label, palette='dark', errorbar='sd' if std else None)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.gcf().set_size_inches(width, height)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_f1_df(model_device, test_devices):\n",
    "    data = {'Device': [], 'F1 Score': [], 'model_type': [], 'cv': []}\n",
    "    df = pd.DataFrame(data)\n",
    "    for device in test_devices:\n",
    "        print(device)\n",
    "        for i in range(8):\n",
    "            new_row = {'Device': device,\n",
    "                       'F1 Score': get_result_for_model(root_path, model_device, device, 'attend', i)[3],\n",
    "                       'model_type': 'attend',\n",
    "                       'cv': i}\n",
    "            df = pd.concat([df, pd.DataFrame(new_row, index=[0])], ignore_index=True)\n",
    "            new_row = {'Device': device,\n",
    "                       'F1 Score': get_result_for_model(root_path, model_device, device, 'deepconv', i)[3],\n",
    "                       'model_type': 'deepconv',\n",
    "                       'cv': i}\n",
    "            df = pd.concat([df, pd.DataFrame(new_row, index=[0])], ignore_index=True)\n",
    "            new_row = {'Device': device,\n",
    "                       'F1 Score': get_result_for_model(root_path, model_device, device, 'tinyhar', i)[3],\n",
    "                       'model_type': 'tinyhar',\n",
    "                       'cv': i}\n",
    "            df = pd.concat([df, pd.DataFrame(new_row, index=[0])], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_graph_with_std(root_path, model_device, test_devices):\n",
    "    df = get_f1_df(model_device, test_devices)\n",
    "\n",
    "    # generate the graph\n",
    "    g = sns.catplot(x='Device', y='F1 Score', hue='model_type', data=df, kind='bar', palette='dark', errorbar='sd',\n",
    "                    alpha=.6, height=6, aspect=2).set(title='F1 Score for Model trained with ' + model_device)\n",
    "    g.despine(left=True)\n",
    "    g.set_ylabels('F1 Score')\n",
    "    g.set_xlabels('Device')\n",
    "    g.set_xticklabels(rotation=45)\n",
    "\n",
    "    # save the graph in folder mean_graphs in the logs, create folder if not exists\n",
    "    if not os.path.exists(os.path.join(root_path, 'data', 'Run_logs', model_device, 'logs', 'mean_graphs')):\n",
    "        os.makedirs(os.path.join(root_path, 'data', 'Run_logs', model_device, 'logs', 'mean_graphs'))\n",
    "    # plt.savefig(os.path.join(root_path, 'data', 'Run_logs', model_device, 'logs', 'mean_graphs',\n",
    "    #                          'F1 Score for Model trained with ' + model_device + '.png'))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-11T10:25:29.832941Z",
     "end_time": "2024-03-11T10:25:29.924467Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_devices = ['bluesense-RWR1', 'bluesense-RWR2', 'bluesense-LWR', 'bluesense-LUA', 'bluesense-RUA', 'bluesense-TRS',\n",
    "                'empatica-left', 'empatica-right', 'maxim-green']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-11T10:25:29.841944Z",
     "end_time": "2024-03-11T10:25:29.925467Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# generate_graph_with_std(root_path, sensor1', test_devices)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-11T10:25:29.856939Z",
     "end_time": "2024-03-11T10:25:29.925467Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MMD Graphs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_delta_mmd_graph(device1, device2):\n",
    "    '''\n",
    "        The graph is generated between the mmd difference between two device for each cv:\n",
    "        For example:\n",
    "        With variation = mmd of (cv2): device2 p001 and device1 (all_p0* - p001)\n",
    "        Without variation = mmd of (cv2): device1 p001 and device1 (all_p0* - p001)\n",
    "\n",
    "        VS\n",
    "\n",
    "        The f1 score difference between two train test set:\n",
    "        Set 1: Train = device1 (all_p0* - p001) Test = device1 p001\n",
    "        Set 2: Train = device1 (all_p0* - p001) Test = device2 p001\n",
    "    '''\n",
    "    # read the mmd values from the csv\n",
    "    path = os.path.join(root_path, 'data', 'mmd', 'mmd_results_' + device1 + '_' + device1 + '.csv')\n",
    "    df_1_1 = pd.read_csv(path)\n",
    "    # print(df_1_1)\n",
    "    path = os.path.join(root_path, 'data', 'mmd', 'mmd_results_' + device2 + '_' + device1 + '.csv')\n",
    "    df_2_1 = pd.read_csv(path)\n",
    "    # print(df_2_1)\n",
    "    # get the magnitude from Acc_X, Acc_Y, Acc_Z for both df\n",
    "    df_1_1['magnitude'] = ((df_1_1['Acc_X']) ** 2 + (df_1_1['Acc_Y']) ** 2 + (\n",
    "        df_1_1['Acc_Z']) ** 2) ** 0.5\n",
    "    df_2_1['magnitude'] = ((df_2_1['Acc_X']) ** 2 + (df_2_1['Acc_Y']) ** 2 + (\n",
    "        df_2_1['Acc_Z']) ** 2) ** 0.5\n",
    "\n",
    "    delta_mag = (df_2_1['magnitude'] - df_1_1['magnitude'])\n",
    "    # rename the column\n",
    "    delta_mag = delta_mag.rename('delta_mag')\n",
    "    f1_1_1 = get_f1_df(device1, [device1])\n",
    "    f1_2_1 = get_f1_df(device2, [device1])\n",
    "    # mean all values with same value in column cv\n",
    "    mean_f1_1_1 = f1_1_1.groupby('cv')['F1 Score'].mean().reset_index()\n",
    "    mean_f1_2_1 = f1_2_1.groupby('cv')['F1 Score'].mean().reset_index()\n",
    "\n",
    "    # drop the cv column\n",
    "    mean_f1_1_1 = mean_f1_1_1.drop(columns=['cv'])\n",
    "    mean_f1_2_1 = mean_f1_2_1.drop(columns=['cv'])\n",
    "    delta_mean_f1 = mean_f1_2_1 - mean_f1_1_1\n",
    "\n",
    "    # print size of delta_mag and mean_f1\n",
    "    delta_mean_f1.iloc[0].iloc[0]\n",
    "\n",
    "    # plot a graph between delta_mag and mean_f1\n",
    "    plt.scatter(delta_mag, delta_mean_f1, label='train-test')\n",
    "    # label each point from 1 to 8\n",
    "    for i in range(8):\n",
    "        plt.annotate(i, (delta_mag[i], delta_mean_f1.iloc[i]))\n",
    "    plt.xlabel('delta_mmd')\n",
    "    plt.ylabel('delta_mean_f1')\n",
    "    # set max y to 0.5 and min y to -0.5\n",
    "    # plt.ylim(-1, 1)\n",
    "    plt.title('delta_mmd vs delta_mean_f1 for ' + device1 + ' and ' + device2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-11T10:25:29.877471Z",
     "end_time": "2024-03-11T10:25:29.925467Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
